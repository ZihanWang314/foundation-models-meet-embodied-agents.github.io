Time,Program
09:00-09:05,Opening Remarks
09:05-09:40,"Keynote Speech <b>Peter Clark: What do our Machines Believe?</b><br><i>Do language models form anything like a 'mental model' when reasoning? And do they have coherent 'beliefs' about the world? Probing an LM, we find the LLM's world views are only partially coherent, and often contain blatent inconsistencies. Taking this further, I'll describe how we can extract 'belief graphs' from LMs and repair the inconsistencies they uncover. More generally, I'll promote a two-layered architecture for future systems, consisting of the LM plus a symbolic representation of (parts of) the model's belief state, supporting systematic reasoning, interaction, addition of external knowledge, and more rational behavior by our future LM companions.</i>"
09:40-10:15,"Keynote Speech <b>Luke Zettlemoyer: Chameleon: Universal Mixed-modal Modeling by Tokenizing Everything</b><br><i>Existing multimodal models typically have custom architectures that are designed for specific modalities (image->text, text->image, text only, etc). In this talk, I will present our recent work on Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. The key idea is to tokenize images into a discrete space, wherever they appear within multimodal documents, and then model the resulting sequences of mixed-modal tokens with a single unified transformer. This approach allows us to trivial lift all of the advanced modeling techniques originally developed for text-only models to the multimodal setting, including multi-task alignment and retrieval augmentation, as I will show. It also performs well overall, demonstrating broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model.</i>"
10:15-10:50,Keynote Speech Tatsu Hashimoto 
10:50-11:05,Coffee Break
11:05-12:25,Oral Presentation: Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs 
,Oral Presentation: AcKnowledge: Acquired Knowledge Representation by Small Language Model Without Pre-training 
,Oral Presentation: Unified Hallucination Detection for Multimodal Large Language Models
,Oral Presentation: Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval 
,Oral Presentation: Measuring the Inconsistency of Large Language Models in Preferential Ranking 
,"Oral Presentation: Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations"
12:25-12:30,Best Paper and Outstanding Paper Announcement
12:30-13:30,Lunch Break
13:30-14:05,"Keynote Speech <b>Isabelle Augenstein: Revealing the Parametric Knowledge of Language Models</b><br><i>Language Models acquire parametric knowledge from their training process, embedding it within their weights. The increasing scalability of LMs, however, poses significant challenges for understanding a model's inner workings and further for updating or correcting this embedded knowledge without the significant cost of retraining. Moreover, when using these language models for knoweldge-intensive language understanding tasks, LMs have to integrate relevant context, mitigating their inherent weaknesses, such as incomplete or outdated knowledge. Nevertheless, studies indicate that LMs often ignore the provided context as it can be in conflict with the pre-existing LM's memory learned during pre-training. Conflicting knowledge can also already be present in the LM's parameters, termed intra-memory conflict. This underscores the importance of unveiling exactly what knowledge is stored and its association with specific model components, and how this knowledge is used for downstream tasks. In this talk, I will present our research on evaluating the knowledge present in LMs, through a unified knowledge attribution framework, as well as diagnostic tests that can reveal knowledge conflicts.</i>"
14:05-14:40,Keynote Speech Eduard Hovy 
14:40-15:15,"Keynote Speech <b>Hannah Rashkin: Challenges in measuring attribution in NLG models</b><br><i>Large language models frequently 'hallucinate' information, making claims about the real world that are not supported by background knowledge. I will discuss our recent work, which explores metrics for attribution, a framework measuring how well information in LLM output is supported by external documents.  I will cover our efforts to measure attribution in knowledge-grounded tasks using both human annotators and automatic metrics.  Lastly, I will talk about ongoing challenges in measuring attribution and areas in which these metrics need further exploration.</i>"
15:15-15:50,Panel Discussion
16:00-17:30,Poster Session